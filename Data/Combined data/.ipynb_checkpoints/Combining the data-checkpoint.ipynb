{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6fd29e1",
   "metadata": {},
   "source": [
    "# Combining the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "0f02641b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Display the first few rows of each DataFrame as scrollable tables in Jupyter Notebook\n",
    "from IPython.display import display\n",
    "from tabulate import tabulate\n",
    "import holidays\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6373caa0",
   "metadata": {},
   "source": [
    "#### Types of information\n",
    "Now that we've examined both the energy data and the forecasting data, there's only one thing left to do: combine them. What we aim for is a final dataset that can be integrated into various other datasets with minimal changes. This dataset should contain three types of information (possibly four, but more on that later):\n",
    "\n",
    "1. Energy consumption data\n",
    "2. Temperature forecasting data\n",
    "3. Information about the day and whether it's a holiday\n",
    "\n",
    "Furthermore, these data sets need to be synchronized in terms of time. The energy data is collected hourly, while the temperature data is gathered every six hours. Both will be aggregated to daily data, but in different ways. The temperature will be averaged throughout the day—though we might consider using the sum, as this would eliminate negative temperatures, which could be interesting to explore. The energy data will be aggregated by summing the data.\n",
    "\n",
    "#### Missing data issue\n",
    "Then there's the issue of missing data, which is why we might need a fourth type of information. One approach is to use linear interpolation to fill in the gaps. While this is a reliable method for handling missing data, it does introduce some uncertainty. If there are long periods of missing data, this could significantly affect the variance of the models. Another approach is to use a flagging method, where we add a variable that flags dates with missing information as \"1\" and days with complete data as \"0\". This could be particularly useful for methods with coefficients, like OLS, as well as for methods like Random Forest Regression. In the latter case, the flag would help the model predict a \"0\" on days where we don't know the actual consumption, thereby providing success metrics based solely on the model's ability to predict known data. We will create datasets for both scenarios.\n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff32821d",
   "metadata": {},
   "source": [
    "## Energy data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643a53ec",
   "metadata": {},
   "source": [
    "### Loading Energy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "2dfc962a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Electricity Consumption Data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HourUTC</th>\n",
       "      <th>HourDK</th>\n",
       "      <th>PriceArea</th>\n",
       "      <th>CentralPowerMWh</th>\n",
       "      <th>LocalPowerMWh</th>\n",
       "      <th>CommercialPowerMWh</th>\n",
       "      <th>LocalPowerSelfConMWh</th>\n",
       "      <th>OffshoreWindLt100MW_MWh</th>\n",
       "      <th>OffshoreWindGe100MW_MWh</th>\n",
       "      <th>OnshoreWindLt50kW_MWh</th>\n",
       "      <th>...</th>\n",
       "      <th>ExchangeNO_MWh</th>\n",
       "      <th>ExchangeSE_MWh</th>\n",
       "      <th>ExchangeGE_MWh</th>\n",
       "      <th>ExchangeNL_MWh</th>\n",
       "      <th>ExchangeGreatBelt_MWh</th>\n",
       "      <th>GrossConsumptionMWh</th>\n",
       "      <th>GridLossTransmissionMWh</th>\n",
       "      <th>GridLossInterconnectorsMWh</th>\n",
       "      <th>GridLossDistributionMWh</th>\n",
       "      <th>PowerToHeatMWh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2005-03-25T22:00:00</td>\n",
       "      <td>2005-03-25T23:00:00</td>\n",
       "      <td>DK1</td>\n",
       "      <td>917.400024</td>\n",
       "      <td>760.206787</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.857113</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022235</td>\n",
       "      <td>...</td>\n",
       "      <td>496.000000</td>\n",
       "      <td>-97.099998</td>\n",
       "      <td>-297.700012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1842.515015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2005-03-25T22:00:00</td>\n",
       "      <td>2005-03-25T23:00:00</td>\n",
       "      <td>DK2</td>\n",
       "      <td>920.900024</td>\n",
       "      <td>271.390656</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.329244</td>\n",
       "      <td>2.978000</td>\n",
       "      <td>0.006352</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>386.600006</td>\n",
       "      <td>-251.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1386.037964</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2005-03-25T21:00:00</td>\n",
       "      <td>2005-03-25T22:00:00</td>\n",
       "      <td>DK1</td>\n",
       "      <td>1079.099976</td>\n",
       "      <td>772.546753</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.533219</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015721</td>\n",
       "      <td>...</td>\n",
       "      <td>808.599976</td>\n",
       "      <td>169.699997</td>\n",
       "      <td>-870.299988</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2010.311157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2005-03-25T21:00:00</td>\n",
       "      <td>2005-03-25T22:00:00</td>\n",
       "      <td>DK2</td>\n",
       "      <td>908.099976</td>\n",
       "      <td>296.979065</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.279539</td>\n",
       "      <td>44.057999</td>\n",
       "      <td>0.008078</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>631.500000</td>\n",
       "      <td>-447.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1501.229004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2005-03-25T20:00:00</td>\n",
       "      <td>2005-03-25T21:00:00</td>\n",
       "      <td>DK1</td>\n",
       "      <td>1125.400024</td>\n",
       "      <td>833.091309</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.346528</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012367</td>\n",
       "      <td>...</td>\n",
       "      <td>991.400024</td>\n",
       "      <td>431.600006</td>\n",
       "      <td>-1245.199951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2180.373779</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               HourUTC               HourDK PriceArea  CentralPowerMWh  \\\n",
       "0  2005-03-25T22:00:00  2005-03-25T23:00:00       DK1       917.400024   \n",
       "1  2005-03-25T22:00:00  2005-03-25T23:00:00       DK2       920.900024   \n",
       "2  2005-03-25T21:00:00  2005-03-25T22:00:00       DK1      1079.099976   \n",
       "3  2005-03-25T21:00:00  2005-03-25T22:00:00       DK2       908.099976   \n",
       "4  2005-03-25T20:00:00  2005-03-25T21:00:00       DK1      1125.400024   \n",
       "\n",
       "   LocalPowerMWh  CommercialPowerMWh  LocalPowerSelfConMWh  \\\n",
       "0     760.206787                 NaN                   0.0   \n",
       "1     271.390656                 NaN                   0.0   \n",
       "2     772.546753                 NaN                   0.0   \n",
       "3     296.979065                 NaN                   0.0   \n",
       "4     833.091309                 NaN                   0.0   \n",
       "\n",
       "   OffshoreWindLt100MW_MWh  OffshoreWindGe100MW_MWh  OnshoreWindLt50kW_MWh  \\\n",
       "0                 4.857113                 0.000000               0.022235   \n",
       "1                 3.329244                 2.978000               0.006352   \n",
       "2                 4.533219                 0.000000               0.015721   \n",
       "3                 5.279539                44.057999               0.008078   \n",
       "4                 3.346528                 0.000000               0.012367   \n",
       "\n",
       "   ...  ExchangeNO_MWh  ExchangeSE_MWh  ExchangeGE_MWh  ExchangeNL_MWh  \\\n",
       "0  ...      496.000000      -97.099998     -297.700012             0.0   \n",
       "1  ...             NaN      386.600006     -251.000000             NaN   \n",
       "2  ...      808.599976      169.699997     -870.299988             0.0   \n",
       "3  ...             NaN      631.500000     -447.000000             NaN   \n",
       "4  ...      991.400024      431.600006    -1245.199951             0.0   \n",
       "\n",
       "   ExchangeGreatBelt_MWh  GrossConsumptionMWh  GridLossTransmissionMWh  \\\n",
       "0                    0.0          1842.515015                      0.0   \n",
       "1                    0.0          1386.037964                      0.0   \n",
       "2                    0.0          2010.311157                      0.0   \n",
       "3                    0.0          1501.229004                      0.0   \n",
       "4                    0.0          2180.373779                      0.0   \n",
       "\n",
       "   GridLossInterconnectorsMWh  GridLossDistributionMWh  PowerToHeatMWh  \n",
       "0                         NaN                      NaN             0.0  \n",
       "1                         NaN                      NaN             0.0  \n",
       "2                         NaN                      NaN             0.0  \n",
       "3                         NaN                      NaN             0.0  \n",
       "4                         NaN                      NaN             0.0  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the electricity consumption dataset\n",
    "consumption_filepath = 'C:/Users/madsh/OneDrive/Dokumenter/kandidat/Fællesmappe/Speciale/Forecasting-energy-consumption-in-Denmark/Data/Energy/Production and Consumption - Settlement.csv'\n",
    "consumption_df = pd.read_csv(consumption_filepath)\n",
    "\n",
    "# Display the electricity consumption DataFrame\n",
    "print(\"Electricity Consumption Data:\")\n",
    "display(consumption_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da006908",
   "metadata": {},
   "source": [
    "#### Electricity Consumption Data\n",
    "* `HourUTC` and `HourDK`: Timestamps in UTC and Danish time\n",
    "* `PriceArea`: Price area (either DK1 or DK2)\n",
    "* `CentralPowerMWh`, `LocalPowerMWh`, etc.: Various measures of electricity consumption and production\n",
    "* `GrossConsumptionMWh`: The measure we are interested in for electricity consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "4538740b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HourDK</th>\n",
       "      <th>GrossConsumptionMWh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2005-01-01 00:00:00</td>\n",
       "      <td>3370.256592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2005-01-01 01:00:00</td>\n",
       "      <td>3237.832763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2005-01-01 02:00:00</td>\n",
       "      <td>3101.580811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2005-01-01 03:00:00</td>\n",
       "      <td>2963.392211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2005-01-01 04:00:00</td>\n",
       "      <td>2854.805420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               HourDK  GrossConsumptionMWh\n",
       "0 2005-01-01 00:00:00          3370.256592\n",
       "1 2005-01-01 01:00:00          3237.832763\n",
       "2 2005-01-01 02:00:00          3101.580811\n",
       "3 2005-01-01 03:00:00          2963.392211\n",
       "4 2005-01-01 04:00:00          2854.805420"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the relevant columns to datetime format\n",
    "consumption_df['HourDK'] = pd.to_datetime(consumption_df['HourDK'])\n",
    "\n",
    "# Summing up GrossConsumptionMWh for both DK1 and DK2 for each time slot\n",
    "consumption_grouped_df = consumption_df.groupby('HourDK')['GrossConsumptionMWh'].sum().reset_index()\n",
    "\n",
    "# Show the first few rows of the grouped DataFrame\n",
    "consumption_grouped_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48112c64",
   "metadata": {},
   "source": [
    "### Linear interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "726dd74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values before interpolation: 1531\n"
     ]
    }
   ],
   "source": [
    "# Generate a complete date range\n",
    "complete_date_range = pd.date_range(start=consumption_grouped_df['HourDK'][0], end=consumption_grouped_df['HourDK'][len(consumption_grouped_df['HourDK'])-1], freq='H')\n",
    "\n",
    "# Put HourDK as DataFrame index\n",
    "consumption_grouped_df.set_index('HourDK', inplace=True)\n",
    "\n",
    "# Reindex the DataFrame to include all dates and set NaN for missing dates\n",
    "consumption_grouped_df = consumption_grouped_df.reindex(complete_date_range)\n",
    "\n",
    "missing_before = consumption_grouped_df['GrossConsumptionMWh'].isna().sum()\n",
    "print(f\"Number of missing values before interpolation: {missing_before}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "16f04990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     GrossConsumptionMWh\n",
      "2005-03-26 00:00:00                  NaN\n",
      "2005-03-26 01:00:00                  NaN\n",
      "2005-03-26 02:00:00                  NaN\n",
      "2005-03-26 03:00:00                  NaN\n",
      "2005-03-26 04:00:00                  NaN\n",
      "...                                  ...\n",
      "2023-04-09 19:00:00                  NaN\n",
      "2023-04-09 20:00:00                  NaN\n",
      "2023-04-09 21:00:00                  NaN\n",
      "2023-04-09 22:00:00                  NaN\n",
      "2023-04-09 23:00:00                  NaN\n",
      "\n",
      "[1531 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Reset index to make 'HourDK' a column again\n",
    "line_numbers = consumption_grouped_df.index[consumption_grouped_df['GrossConsumptionMWh'].isna()].tolist()\n",
    "selected_rows = consumption_grouped_df.loc[line_numbers]\n",
    "print(selected_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "5011e73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     GrossConsumptionMWh\n",
      "2005-03-26 00:00:00          3219.117071\n",
      "2005-03-26 01:00:00          3209.681163\n",
      "2005-03-26 02:00:00          3200.245254\n",
      "2005-03-26 03:00:00          3190.809346\n",
      "2005-03-26 04:00:00          3181.373438\n",
      "...                                  ...\n",
      "2023-04-09 19:00:00          2976.840017\n",
      "2023-04-09 20:00:00          2974.051890\n",
      "2023-04-09 21:00:00          2971.263764\n",
      "2023-04-09 22:00:00          2968.475637\n",
      "2023-04-09 23:00:00          2965.687510\n",
      "\n",
      "[1531 rows x 1 columns]\n",
      "                    HourDK  GrossConsumptionMWh\n",
      "0      2005-01-01 00:00:00          3370.256592\n",
      "1      2005-01-01 01:00:00          3237.832763\n",
      "2      2005-01-01 02:00:00          3101.580811\n",
      "3      2005-01-01 03:00:00          2963.392211\n",
      "4      2005-01-01 04:00:00          2854.805420\n",
      "...                    ...                  ...\n",
      "161371 2023-05-30 19:00:00          3935.964505\n",
      "161372 2023-05-30 20:00:00          3764.163099\n",
      "161373 2023-05-30 21:00:00          3655.639568\n",
      "161374 2023-05-30 22:00:00          3663.715933\n",
      "161375 2023-05-30 23:00:00          3308.564927\n",
      "\n",
      "[161376 rows x 2 columns]\n",
      "\n",
      "Number of missing values after interpolation: 0\n"
     ]
    }
   ],
   "source": [
    "# Perform linear interpolation\n",
    "combined_daily_interpolation_df = consumption_grouped_df.interpolate(method='linear')\n",
    "\n",
    "line_numbers = consumption_grouped_df.index[consumption_grouped_df['GrossConsumptionMWh'].isna()].tolist()\n",
    "selected_rows = combined_daily_interpolation_df.loc[line_numbers]\n",
    "print(selected_rows)\n",
    "\n",
    "# Reset index to make 'HourDK' a column again\n",
    "combined_daily_interpolation_df.reset_index(inplace=True)\n",
    "combined_daily_interpolation_df.rename(columns={'index': 'HourDK'}, inplace=True)\n",
    "\n",
    "# Print the DataFrame to check the results\n",
    "print(combined_daily_interpolation_df)\n",
    "\n",
    "# Count and print the number of missing values after interpolation\n",
    "missing_after = combined_daily_interpolation_df['GrossConsumptionMWh'].isna().sum()\n",
    "print(f\"\\nNumber of missing values after interpolation: {missing_after}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "743b20fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [DatetimeIndex(['2005-03-26 00:00:00', '2005-03-26 01:00:00',\\n               '2005-03-26 02:00:00', '2005-03-26 03:00:00',\\n               '2005-03-26 04:00:00', '2005-03-26 05:00:00',\\n               '2005-03-26 06:00:00', '2005-03-26 07:00:00',\\n               '2005-03-26 08:00:00', '2005-03-26 09:00:00',\\n               ...\\n               '2023-04-09 14:00:00', '2023-04-09 15:00:00',\\n               '2023-04-09 16:00:00', '2023-04-09 17:00:00',\\n               '2023-04-09 18:00:00', '2023-04-09 19:00:00',\\n               '2023-04-09 20:00:00', '2023-04-09 21:00:00',\\n               '2023-04-09 22:00:00', '2023-04-09 23:00:00'],\\n              dtype='datetime64[ns]', length=1531, freq=None)] are in the [index]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14088\\4278697376.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mline_numbers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconsumption_grouped_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mconsumption_grouped_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'GrossConsumptionMWh'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mselected_rows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcombined_daily_interpolation_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mline_numbers\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mselected_rows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    965\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    966\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 967\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    968\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    969\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1189\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cannot index with multidimensional key\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1191\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_iterable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1193\u001b[0m             \u001b[1;31m# nested tuple slicing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_iterable\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# A collection of keys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1132\u001b[1;33m         \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1133\u001b[0m         return self.obj._reindex_with_indexers(\n\u001b[0;32m   1134\u001b[0m             \u001b[1;33m{\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_dups\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1325\u001b[0m         \u001b[0maxis_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m         \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   5780\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5782\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5783\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5784\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   5840\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0muse_interval_msg\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5841\u001b[0m                     \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5842\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5843\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5844\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [DatetimeIndex(['2005-03-26 00:00:00', '2005-03-26 01:00:00',\\n               '2005-03-26 02:00:00', '2005-03-26 03:00:00',\\n               '2005-03-26 04:00:00', '2005-03-26 05:00:00',\\n               '2005-03-26 06:00:00', '2005-03-26 07:00:00',\\n               '2005-03-26 08:00:00', '2005-03-26 09:00:00',\\n               ...\\n               '2023-04-09 14:00:00', '2023-04-09 15:00:00',\\n               '2023-04-09 16:00:00', '2023-04-09 17:00:00',\\n               '2023-04-09 18:00:00', '2023-04-09 19:00:00',\\n               '2023-04-09 20:00:00', '2023-04-09 21:00:00',\\n               '2023-04-09 22:00:00', '2023-04-09 23:00:00'],\\n              dtype='datetime64[ns]', length=1531, freq=None)] are in the [index]\""
     ]
    }
   ],
   "source": [
    "line_numbers = consumption_grouped_df.index[consumption_grouped_df['GrossConsumptionMWh'].isna()].tolist()\n",
    "selected_rows = combined_daily_interpolation_df.loc[line_numbers]\n",
    "print(selected_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e4b1bf",
   "metadata": {},
   "source": [
    "### Flagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a7f58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumption_df = pd.read_csv(consumption_filepath)\n",
    "\n",
    "# Convert the relevant columns to datetime format\n",
    "consumption_df['HourDK'] = pd.to_datetime(consumption_df['HourDK'])\n",
    "\n",
    "# Summing up GrossConsumptionMWh for both DK1 and DK2 for each time slot\n",
    "consumption_grouped_df['HourDK'] = pd.to_datetime(consumption_df['HourDK'])\n",
    "consumption_grouped_flagged_df = consumption_df.groupby('HourDK')['GrossConsumptionMWh'].sum().reset_index()\n",
    "\n",
    "# Generate a complete date range\n",
    "complete_date_range = pd.date_range(start=consumption_grouped_flagged_df['HourDK'].min(), end=consumption_grouped_flagged_df['HourDK'].max(), freq='H')\n",
    "\n",
    "# Put HourDK as DataFrame index\n",
    "consumption_grouped_flagged_df.set_index('HourDK', inplace=True)\n",
    "\n",
    "# Reindex the DataFrame to include all dates and set NaN for missing dates\n",
    "consumption_grouped_flagged_df = consumption_grouped_flagged_df.reindex(complete_date_range)\n",
    "\n",
    "# Check the number of missing values before flagging\n",
    "missing_before = consumption_grouped_flagged_df['GrossConsumptionMWh'].isna().sum()\n",
    "print(f\"Number of missing values before flagging: {missing_before}\")\n",
    "\n",
    "# Create the 'flagged' column\n",
    "consumption_grouped_flagged_df['flagged'] = np.where(\n",
    "    (consumption_grouped_flagged_df['GrossConsumptionMWh'] < 1) | \n",
    "    consumption_grouped_flagged_df['GrossConsumptionMWh'].isna(), \n",
    "    1, \n",
    "    0\n",
    ")\n",
    "\n",
    "# Reset the index to make HourDK a column again and remove the duplicate index\n",
    "consumption_grouped_flagged_df.reset_index(inplace=True)\n",
    "consumption_grouped_flagged_df.rename(columns={'index': 'HourDK'}, inplace=True)\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "print(consumption_grouped_flagged_df)\n",
    "\n",
    "# Count and print the number of missing values after flagging\n",
    "missing_after = consumption_grouped_flagged_df['GrossConsumptionMWh'].isna().sum()\n",
    "print(f\"\\nNumber of missing values after Flagging: {missing_after}\")\n",
    "print(f\"Number of 'Flagged' observations: {sum(consumption_grouped_flagged_df['flagged'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bde6dc",
   "metadata": {},
   "source": [
    "### Aggregating to daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adc2fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample the interpolated electricity consumption data to daily level and sum up GrossConsumptionMWh\n",
    "combined_daily_interpolation_df = combined_daily_interpolation_df.resample('D', on='HourDK').sum().reset_index()\n",
    "\n",
    "# Show the first few rows of the daily aggregated DataFrame\n",
    "print(\"Interpolated energy data:\")\n",
    "print(combined_daily_interpolation_df)\n",
    "\n",
    "# Resample the interpolated electricity consumption data to daily level and sum up GrossConsumptionMWh\n",
    "consumption_daily_flagged_df = consumption_grouped_flagged_df.resample('D', on='HourDK').agg({'GrossConsumptionMWh': 'sum', 'flagged': 'max'}).reset_index()\n",
    "consumption_daily_df['GrossConsumptionMWh'] = np.where(consumption_daily_flagged_df['flagged'] == 1, 0, consumption_daily_flagged_df['GrossConsumptionMWh'])\n",
    "# Show the first few rows of the daily aggregated DataFrame\n",
    "print(\"\\nFlagged energy data:\")\n",
    "print(consumption_daily_flagged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec55e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "line_numbers = combined_daily_flagged_df.index[combined_daily_flagged_df['flagged'] == 1].tolist()\n",
    "selected_rows = combined_daily_interpolation_df.loc[line_numbers]\n",
    "selected_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cb641e",
   "metadata": {},
   "source": [
    "Now that the energy data has been prepared for integration into a combined dataset, let's now prepare the weather forecast data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2579cd21",
   "metadata": {},
   "source": [
    "## Weather Forecast Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877603d2",
   "metadata": {},
   "source": [
    "### Loading Energy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43e8f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the weather forecast dataset\n",
    "weather_filepath = 'C:/Users/madsh/OneDrive/Dokumenter/kandidat/Fællesmappe/Speciale/Forecasting-energy-consumption-in-Denmark/Data/Weather forecasts/combined_forecasts_2005-2023.csv'\n",
    "weather_df = pd.read_csv(weather_filepath)\n",
    "\n",
    "# Display the weather forecast DataFrame\n",
    "print(\"Weather Forecast Data:\")\n",
    "display(weather_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b6a64f",
   "metadata": {},
   "source": [
    "#### Weather Data\n",
    "* `valid_time`: The valid time for the weather forecast\n",
    "* `time`: The date of the forecast\n",
    "* `step`: The step length for the forecast (e.g., \"0 days 06:00:00\" means 6 hours ahead)\n",
    "* `t2m`: Temperature in degrees Celsius"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790e3b5c",
   "metadata": {},
   "source": [
    "### Aggregating to Daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82707d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df['valid_time'] = pd.to_datetime(weather_df['valid_time'])\n",
    "\n",
    "# Resample the weather data to daily level and average the temperature (t2m)\n",
    "weather_daily_df = weather_df.resample('D', on='valid_time').mean().reset_index()\n",
    "\n",
    "# Show the first few rows of the daily aggregated DataFrame\n",
    "weather_daily_df.head()\n",
    "\n",
    "# Filter out the 00:00 observations for each day to keep the 'step' information\n",
    "weather_step_df = weather_df[weather_df['valid_time'].dt.hour == 0]\n",
    "\n",
    "# Merge the filtered 'step' DataFrame with the daily averaged temperature DataFrame on 'valid_time'\n",
    "weather_daily_with_step_df = pd.merge(weather_daily_df, weather_step_df[['valid_time', 'step']], on='valid_time', how='left')\n",
    "\n",
    "# Set the 'step' for the first day of the year to be \"0 days\"\n",
    "weather_daily_with_step_df.loc[0, 'step'] = pd.Timedelta(days=0)\n",
    "weather_daily_with_step_df['step'].fillna(pd.Timedelta('0 days 00:00:00'), inplace=True)\n",
    "\n",
    "\n",
    "# Show the first few rows of the daily aggregated DataFrame with 'step'\n",
    "weather_daily_with_step_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6f58cf",
   "metadata": {},
   "source": [
    "#### Convert step to numerical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bd6186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, convert 'step' to string\n",
    "weather_daily_with_step_df['step'] = weather_daily_with_step_df['step'].astype(str)\n",
    "\n",
    "# Extract the number of days and convert to float first (this will convert 'nan' to NaN)\n",
    "weather_daily_with_step_df['step_days'] = weather_daily_with_step_df['step'].str.split(' ').str[0].astype(float)\n",
    "\n",
    "# Now filter out the rows where 'step_days' is NaN if you want\n",
    "weather_daily_with_step_df = weather_daily_with_step_df[weather_daily_with_step_df['step_days'].notna()]\n",
    "\n",
    "# Finally, convert to integer\n",
    "weather_daily_with_step_df['step_days'] = weather_daily_with_step_df['step_days'].astype(int)\n",
    "\n",
    "# Drop the original 'step' column\n",
    "weather_daily_with_step_df.drop('step', axis=1, inplace=True)\n",
    "\n",
    "weather_daily_with_step_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b44783",
   "metadata": {},
   "source": [
    "Now that we have prepared the weather forecasting data, we can combine the two datasets. Again, we will create two datasets: one for interpolation and one for flagging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f26b4c4",
   "metadata": {},
   "source": [
    "## Combining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04c9a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the daily aggregated electricity consumption data with the daily aggregated weather data\n",
    "combined_daily_interpolation_df = pd.merge(consumption_daily_df, weather_daily_with_step_df, \n",
    "                             left_on='HourDK', right_on='valid_time', how='inner')\n",
    "\n",
    "# Drop the redundant 'valid_time' column\n",
    "combined_daily_interpolation_df.drop('valid_time', axis=1, inplace=True)\n",
    "\n",
    "# Show the first few rows of the combined DataFrame\n",
    "print(\"Interpolated combined data:\")\n",
    "display(combined_daily_interpolation_df.head())\n",
    "\n",
    "# Merge the daily aggregated electricity consumption data with the daily aggregated weather data\n",
    "combined_daily_flagged_df = pd.merge(consumption_daily_flagged_df, weather_daily_with_step_df, \n",
    "                             left_on='HourDK', right_on='valid_time', how='inner')\n",
    "\n",
    "# Drop the redundant 'valid_time' column\n",
    "combined_daily_flagged_df.drop('valid_time', axis=1, inplace=True)\n",
    "\n",
    "# Show the first few rows of the combined DataFrame\n",
    "print(\"\\nFlagged combined data:\")\n",
    "display(combined_daily_flagged_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63f6163",
   "metadata": {},
   "source": [
    "Now that we have combined the datasets, it is time to add the information about what day it is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36943fd2",
   "metadata": {},
   "source": [
    "## Adding additional variable for charateristics for the days and holidays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3113aa4e",
   "metadata": {},
   "source": [
    "To enrich the dataset, we'll add the following features:\n",
    "\n",
    "1. **Day of the Week**: A categorical variable representing which day of the week a given date falls on.\n",
    "2. **Month of the Year**: A categorical variable representing the month in which a given date falls.\n",
    "3. **Holiday Status**: A binary variable indicating whether a given date is a public holiday in Denmark or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4eaafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 'Day_of_Week' and 'Month_of_Year' columns\n",
    "combined_daily_interpolation_df['Day_of_Week'] = combined_daily_interpolation_df['HourDK'].dt.day_name()\n",
    "combined_daily_interpolation_df['Month_of_Year'] = combined_daily_interpolation_df['HourDK'].dt.month_name()\n",
    "\n",
    "# Show the first few rows of the combined DataFrame\n",
    "print(\"Interpolated combined data:\")\n",
    "display(combined_daily_interpolation_df.head())\n",
    "\n",
    "# Add 'Day_of_Week' and 'Month_of_Year' columns\n",
    "combined_daily_flagged_df['Day_of_Week'] = combined_daily_flagged_df['HourDK'].dt.day_name()\n",
    "combined_daily_flagged_df['Month_of_Year'] = combined_daily_flagged_df['HourDK'].dt.month_name()\n",
    "\n",
    "# Show the first few rows to confirm the added columns\n",
    "print(\"\\nFlagged combined data:\")\n",
    "display(combined_daily_flagged_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22b68e7",
   "metadata": {},
   "source": [
    "### Holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d895d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the list of holidays for Denmark for the years 2005-2023\n",
    "danish_holidays = [date for year in range(2005, 2024) for date, _ in holidays.Denmark(years=year).items()]\n",
    "# Convert the list to a pandas Series and make sure it's in datetime format\n",
    "danish_holidays = pd.Series(pd.to_datetime(danish_holidays))\n",
    "# Convert the holiday dates to datetime format\n",
    "danish_holidays = pd.to_datetime(danish_holidays)\n",
    "\n",
    "# Create a new column 'Is_Holiday' and set it to 1 if the date is a holiday, 0 otherwise\n",
    "combined_daily_interpolation_df['Is_Holiday'] = combined_daily_interpolation_df['HourDK'].isin(danish_holidays).astype(int)\n",
    "combined_daily_flagged_df['Is_Holiday'] = combined_daily_flagged_df['HourDK'].isin(danish_holidays).astype(int)\n",
    "\n",
    "# Show the first few rows of the combined DataFrame\n",
    "print(\"Interpolated combined data:\")\n",
    "display(combined_daily_interpolation_df.head())\n",
    "\n",
    "# Show the first few rows to confirm the added columns\n",
    "print(\"\\nFlagged combined data:\")\n",
    "display(combined_daily_flagged_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2514a241",
   "metadata": {},
   "source": [
    "Now we are extremely close to have a final dataset. but there are a couple of steps left before we can save these dataframes as csv files and go nuts with putting the data into machine learning models. \n",
    "\n",
    "First When working with machine learning models for forecasting, using dummy variables instead of categorical variables can offer several advantages. Here's why:\n",
    "\n",
    "#### Interpretability and Consistency Across Models:\n",
    "Different machine learning algorithms handle categorical variables in different ways. Some might not even accept categorical variables as input, requiring pre-processing. Transforming categorical variables into a series of dummy variables (often called \"one-hot encoding\") can make it easier to compare and interpret results across different models.\n",
    "\n",
    "#### Non-linear Relationships:\n",
    "Using dummy variables allows the model to understand non-linear relationships between the categories and the dependent variable, which might not be easily captured if the categorical variables are left as-is or labeled in a numerical but ordinal way.\n",
    "\n",
    "#### Interaction Effects:\n",
    "Dummy variables make it easier to include interaction terms in your model. If you believe that the effect of one variable depends on the level of another variable, dummy variables make it straightforward to model these interactions.\n",
    "\n",
    "#### Model Performance:\n",
    "Last but not least, using dummy variables can actually improve model performance. When a categorical variable is converted to dummy variables, the model has more specific variables to \"play\" with, often resulting in a more accurate and robust model.\n",
    "\n",
    "However, it's worth mentioning that introducing many dummy variables can also lead to the \"curse of dimensionality,\" where the feature space becomes too large, thereby requiring more data for the model to generalize well. But this is often a manageable challenge, and techniques like dimensionality reduction can help if it becomes a problem.\n",
    "\n",
    "So for these reasons, using dummy variables is often preferable to using categorical variables when forecasting with machine learning models. so lets convert the varibels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bc007b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dummy variables for 'Day_of_Week' and 'Month_of_Year'\n",
    "day_of_week_dummies = pd.get_dummies(combined_daily_interpolation_df['Day_of_Week'], prefix='Day')\n",
    "month_of_year_dummies = pd.get_dummies(combined_daily_interpolation_df['Month_of_Year'], prefix='Month')\n",
    "\n",
    "# Concatenate the original DataFrame with the dummy variables\n",
    "combined_daily_interpolation_df = pd.concat([combined_daily_interpolation_df, day_of_week_dummies, month_of_year_dummies], axis=1)\n",
    "combined_daily_flagged_df       = pd.concat([combined_daily_flagged_df,       day_of_week_dummies, month_of_year_dummies], axis=1)\n",
    "\n",
    "# Drop the original 'Day_of_Week' and 'Month_of_Year' columns\n",
    "combined_daily_interpolation_df.drop(['Day_of_Week', 'Month_of_Year'], axis=1, inplace=True)\n",
    "combined_daily_flagged_df.drop(['Day_of_Week', 'Month_of_Year']     , axis=1, inplace=True)\n",
    "\n",
    "# Define the desired column order\n",
    "day_columns = ['Day_Monday', 'Day_Tuesday', 'Day_Wednesday', 'Day_Thursday', 'Day_Friday', 'Day_Saturday', 'Day_Sunday']\n",
    "month_columns = ['Month_January', 'Month_February', 'Month_March', 'Month_April', 'Month_May', 'Month_June', 'Month_July', 'Month_August', 'Month_September', 'Month_October', 'Month_November', 'Month_December']\n",
    "other_columns_inter = ['HourDK', 'GrossConsumptionMWh', 't2m', 'step_days','Is_Holiday']\n",
    "other_columns_flag  = ['HourDK', 'GrossConsumptionMWh','flagged', 't2m', 'step_days','Is_Holiday']\n",
    "\n",
    "# Combine all columns in the desired order\n",
    "ordered_columns_inter = other_columns_inter + day_columns + month_columns\n",
    "ordered_columns_flag = other_columns_flag + day_columns + month_columns\n",
    "\n",
    "# Reorder the columns in the DataFrame\n",
    "combined_daily_interpolation_df = combined_daily_interpolation_df[ordered_columns_inter]\n",
    "combined_daily_flagged_df       = combined_daily_flagged_df[ordered_columns_flag]\n",
    "\n",
    "# Show the first few rows to confirm the new column order\n",
    "print(\"Interpolated combined data:\")\n",
    "display(combined_daily_interpolation_df.head())\n",
    "\n",
    "# Show the first few rows to confirm the added columns\n",
    "print(\"\\nFlagged combined data:\")\n",
    "display(combined_daily_flagged_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c71223",
   "metadata": {},
   "source": [
    "Now there is only one thing left to do: we need to remove all observations from 2023. This is because we are forecasting six months ahead, and the specific day from which we forecast is important. In that spirit, we need the last six months of the dataset to serve as our test set, which means it should end on December 31, 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee698368",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_daily_interpolation_df = combined_daily_interpolation_df[combined_daily_interpolation_df['HourDK'] <= '2022-12-31']\n",
    "combined_daily_flagged_df = combined_daily_flagged_df[combined_daily_flagged_df['HourDK'] <= '2022-12-31']\n",
    "\n",
    "# Show the first few rows to confirm the new column order\n",
    "print(\"Interpolated combined data:\")\n",
    "display(combined_daily_interpolation_df.tail())\n",
    "\n",
    "# Show the first few rows to confirm the added columns\n",
    "print(\"\\nFlagged combined data:\")\n",
    "display(combined_daily_flagged_df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012f76c3",
   "metadata": {},
   "source": [
    "Finally we can export these dataframes as csv files, which can be found on the github!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cab231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export combined_daily_interpolation_df to a CSV file\n",
    "combined_daily_interpolation_df.to_csv('combined_daily_interpolation.csv', index=False)\n",
    "\n",
    "# Export combined_daily_flagged_df to a CSV file\n",
    "combined_daily_flagged_df.to_csv('combined_daily_flagged.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e33e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "line_numbers = combined_daily_flagged_df.index[combined_daily_flagged_df['flagged'] == 1].tolist()\n",
    "selected_rows = combined_daily_interpolation_df.loc[line_numbers]\n",
    "selected_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa3f0dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
